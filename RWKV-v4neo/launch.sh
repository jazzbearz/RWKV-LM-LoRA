#/bin/bash
# python train.py --load_model RWKV-4-Raven-3B-v9-EngOther-20230411-ctx4096.pth --proj_dir trains --data_file train.txt --data_type "utf-8" --vocab_size 50277 --ctx_len 4096 --epoch_steps 100 --epoch_count 1000 --epoch_begin 0 --epoch_save 1 --micro_bsz 11 --n_layer 24 --n_embd 2560 --pre_ffn 0 --head_qk 0 --lr_init 1e-4 --lr_final 1e-4 --warmup_steps 0 --beta1 0.9 --beta2 0.999 --adam_eps 1e-8 --accelerator gpu --devices 1 --precision bf16 --strategy deepspeed_stage_2 --grad_cp 0 --lora --lora_r 8 --lora_alpha 16 --lora_dropout 0.01 --lora_parts=att,time,ln
# python train.py --load_model "RWKV-4-Raven-3B-v9-EngOther-20230411-ctx4096.pth" --wandb "" --proj_dir "out" --data_file "train.txt" --data_type "utf-8" --vocab_size 50277 --ctx_len 1024 --epoch_steps 200 --epoch_count 1000 --epoch_begin 0 --epoch_save 1 --micro_bsz 11 --n_layer 24 --n_embd 2560 --pre_ffn 0 --head_qk 0 --lr_init 1e-5 --lr_final 1e-5 --warmup_steps 0 --beta1 0.9 --beta2 0.999 --adam_eps 1e-8 --accelerator gpu --devices 1 --precision fp16 --strategy deepspeed_stage_2_offload --grad_cp 1 --lora --lora_r 8 --lora_alpha 16 --lora_dropout 0.01 --lora_parts=att,time,ln
# python3 train.py   --load_model <path_to_initial_check_point>   --proj_dir <path_where_checkpoints_will_be_saved>   --data_file ./alpaca_extended.txt(<your data>)   --data_type utf-8   --vocab_size 50277 --ctx_len 384 --epoch_steps 600 --epoch_count 30 --epoch_begin 0 --epoch_save 1 --micro_bsz 2 --n_layer 32 --n_embd 4096 --pre_ffn 0 --head_qk 0 --lr_init 2e-5 --lr_final 5e-7 --warmup_steps 0 --beta1 0.9 --beta2 0.999 --adam_eps 1e-8 --accelerator gpu --devices 4 --precision bf16 --strategy deepspeed_stage_2 --grad_cp 1
# python train.py --load_model RWKV-4-Raven-3B-v9-EngOther-20230411-ctx4096.pth --proj_dir out --data_file train.txt --data_type "utf-8" --vocab_size 50277 --ctx_len 4096 --epoch_steps 100 --epoch_count 1000 --epoch_begin 0 --epoch_save 5 --micro_bsz 11 --n_layer 24 --n_embd 2560 --pre_ffn 0 --head_qk 0 --lr_init 1e-4 --lr_final 1e-4 --warmup_steps 0 --beta1 0.9 --beta2 0.999 --adam_eps 1e-8 --accelerator gpu --devices 1 --precision bf16 --strategy deepspeed_stage_2 --grad_cp 0 --lora --lora_r 8 --lora_alpha 16 --lora_dropout 0.01 --lora_parts=att,time,ln
# python train.py --load_model RWKV-4-Raven-3B-v9-EngOther-20230411-ctx4096.pth --proj_dir out --data_file train.txt --data_type "utf-8" --vocab_size 50277 --ctx_len 256 --epoch_steps 100 --epoch_count 1000 --epoch_begin 0 --epoch_save 5 --micro_bsz 2 --n_layer 24 --n_embd 2560 --pre_ffn 0 --head_qk 0 --lr_init 1e-4 --lr_final 1e-4 --warmup_steps 0 --beta1 0.9 --beta2 0.999 --adam_eps 1e-8 --accelerator gpu --devices 1 --precision bf16 --strategy deepspeed_stage_2 --grad_cp 0 --lora --lora_r 8 --lora_alpha 16 --lora_dropout 0.01 --lora_parts=att,time,ln
# lower testing - python train.py --load_model RWKV-4-Raven-1B5-v8-Eng-20230408-ctx4096.pth --proj_dir out --data_file train.txt --data_type "utf-8" --vocab_size 50277 --ctx_len 256 --epoch_steps 100 --epoch_count 1000 --epoch_begin 0 --epoch_save 5 --micro_bsz 2 --n_layer 24 --n_embd 2048 --pre_ffn 0 --head_qk 0 --lr_init 1e-4 --lr_final 1e-4 --warmup_steps 0 --beta1 0.9 --beta2 0.999 --adam_eps 1e-8 --accelerator gpu --devices 1 --precision bf16 --strategy deepspeed_stage_2 --grad_cp 0 --lora --lora_r 8 --lora_alpha 16 --lora_dropout 0.01 --lora_parts=att,time,ln
# python train.py --load_model RWKV-4-Raven-3B-v9-EngOther-20230411-ctx4096.pth --proj_dir out --data_file train.txt --data_type "utf-8" --vocab_size 50277 --ctx_len 2048 --epoch_steps 100 --epoch_count 1000 --epoch_begin 0 --epoch_save 5 --micro_bsz 6 --n_layer 24 --n_embd 2560 --pre_ffn 0 --head_qk 0 --lr_init 1e-4 --lr_final 1e-4 --warmup_steps 0 --beta1 0.9 --beta2 0.999 --adam_eps 1e-8 --accelerator gpu --devices 1 --precision bf16 --strategy deepspeed_stage_2 --grad_cp 0 --lora --lora_r 8 --lora_alpha 16 --lora_dropout 0.01 --lora_parts=att,time,ln
python train.py --load_model /workspace/RWKV-4-Raven-7B-v8-EngAndMore-20230408-ctx4096.pth --proj_dir /workspace/out --data_file /workspace/train.txt --data_type "utf-8" --vocab_size 50277 --ctx_len 2048 --epoch_steps 100 --epoch_count 1000 --epoch_begin 0 --epoch_save 5 --micro_bsz 1 --n_layer 24 --n_embd 4096 --pre_ffn 0 --head_qk 0 --lr_init 1e-4 --lr_final 1e-4 --warmup_steps 0 --beta1 0.9 --beta2 0.999 --adam_eps 1e-8 --accelerator gpu --devices 1 --precision bf16 --strategy deepspeed_stage_2 --grad_cp 0 --lora --lora_r 8 --lora_alpha 16 --lora_dropout 0.01 --lora_parts=att,time,ln
python train.py --load_model /workspace/RWKV-4-Raven-3B-v8-EngAndMore-20230408-ctx4096.pth --proj_dir /workspace/out --data_file /workspace/train.txt --data_type "utf-8" --vocab_size 50277 --ctx_len 2048 --epoch_steps 100 --epoch_count 1000 --epoch_begin 0 --epoch_save 5 --micro_bsz 3 --n_layer 24 --n_embd 2560 --pre_ffn 0 --head_qk 0 --lr_init 1e-4 --lr_final 1e-4 --warmup_steps 0 --beta1 0.9 --beta2 0.999 --adam_eps 1e-8 --accelerator gpu --devices 1 --precision bf16 --strategy deepspeed_stage_2 --grad_cp 0 --lora --lora_r 8 --lora_alpha 16 --lora_dropout 0.01 --lora_parts=att,time,ln

python train.py --load_model /workspace/RWKV-4-Raven-3B-v8-EngAndMore-20230408-ctx4096.pth --proj_dir /workspace/out --data_file /workspace/train.txt --data_type "utf-8" --vocab_size 50277 --ctx_len 2048 --epoch_steps 100 --epoch_count 1000 --epoch_begin 0 --epoch_save 5 --micro_bsz 3 --n_layer 24 --n_embd 2560 --pre_ffn 0 --head_qk 0 --lr_init 1e-4 --lr_final 1e-4 --warmup_steps 0 --beta1 0.9 --beta2 0.999 --adam_eps 1e-8 --accelerator gpu --devices 1 --precision bf16 --strategy deepspeed_stage_2 --grad_cp 0 --lora --lora_r 8 --lora_alpha 16 --lora_dropout 0.01 --lora_parts=att,time,ln

python train.py --load_model /workspace/RWKV-4-Raven-7B-v8-EngAndMore-20230408-ctx4096.pth --proj_dir /workspace/out --data_file /workspace/train.txt --data_type "utf-8" --vocab_size 50277 --ctx_len 1024 --epoch_steps 100 --epoch_count 1000 --epoch_begin 0 --epoch_save 5 --micro_bsz 1 --n_layer 24 --n_embd 4096 --pre_ffn 0 --head_qk 0 --lr_init 1e-4 --lr_final 1e-4 --warmup_steps 0 --beta1 0.9 --beta2 0.999 --adam_eps 1e-8 --accelerator gpu --devices 2 --precision bf16 --strategy deepspeed_stage_2 --grad_cp 0 --lora --lora_r 8 --lora_alpha 16 --lora_dropout 0.01 --lora_parts=att,time,ln
python train.py --load_model /workspace/RWKV-4-Raven-7B-v8-EngAndMore-20230408-ctx4096.pth --proj_dir /workspace/out --data_file /workspace/train.txt --data_type "utf-8" --vocab_size 50277 --ctx_len 1024 --epoch_steps 100 --epoch_count 1000 --epoch_begin 0 --epoch_save 5 --micro_bsz 1 --n_layer 24 --n_embd 4096 --pre_ffn 0 --head_qk 0 --lr_init 1e-4 --lr_final 1e-4 --warmup_steps 0 --beta1 0.9 --beta2 0.999 --adam_eps 1e-8 --accelerator gpu --devices 2 --precision bf16 --strategy deepspeed_stage_3 --grad_cp 0 --lora --lora_r 8 --lora_alpha 16 --lora_dropout 0.01 --lora_parts=att,time,ln
python train.py --load_model /workspace/RWKV-4-Raven-7B-v8-EngAndMore-20230408-ctx4096.pth --proj_dir /workspace/out --data_file /workspace/train.txt --data_type "utf-8" --vocab_size 50277 --ctx_len 1024 --epoch_steps 100 --epoch_count 1000 --epoch_begin 0 --epoch_save 5 --micro_bsz 1 --n_layer 24 --n_embd 4096 --pre_ffn 0 --head_qk 0 --lr_init 1e-4 --lr_final 1e-4 --warmup_steps 0 --beta1 0.9 --beta2 0.999 --adam_eps 1e-8 --accelerator gpu --devices 2 --precision bf16 --strategy deepspeed_stage_3 --grad_cp 1 --lora --lora_r 8 --lora_alpha 16 --lora_dropout 0.01 --lora_parts=att,time,ln

# single 3090 7B (Over 55GB Ram Required, 22046MiB used)
python train.py --load_model /workspace/RWKV-4-Raven-7B-v8-EngAndMore-20230408-ctx4096.pth --proj_dir /workspace/out --data_file /workspace/train.txt --data_type "utf-8" --vocab_size 50277 --ctx_len 2048 --epoch_steps 100 --epoch_count 1000 --epoch_begin 0 --epoch_save 5 --micro_bsz 1 --n_layer 24 --n_embd 4096 --pre_ffn 0 --head_qk 0 --lr_init 1e-4 --lr_final 1e-4 --warmup_steps 0 --beta1 0.9 --beta2 0.999 --adam_eps 1e-8 --accelerator gpu --devices 1 --precision bf16 --strategy deepspeed_stage_2 --grad_cp 0 --lora --lora_r 8 --lora_alpha 16 --lora_dropout 0.01 --lora_parts=att,time,ln
